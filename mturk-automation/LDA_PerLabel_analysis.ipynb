{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Websites that I basically just copied/followed walkthrough from:\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "from nltk.corpus import stopwords\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "from tkinter import Tk, filedialog\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259e0edf4ff34a38b753b0354fbafeba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectFilesButton(description='Select Files', icon='square-o', style=ButtonStyle(button_color='orange'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SelectFilesButton(widgets.Button):\n",
    "    \"\"\"A file widget that leverages tkinter.filedialog.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize the SelectFilesButton class.\"\"\"\n",
    "        super(SelectFilesButton, self).__init__(*args, **kwargs)\n",
    "        # Add the selected_files trait\n",
    "        self.add_traits(files=traitlets.traitlets.List())\n",
    "        # Create the button.\n",
    "        self.description = \"Select Files\"\n",
    "        self.icon = \"square-o\"\n",
    "        self.style.button_color = \"orange\"\n",
    "        # Set on click behavior.\n",
    "        self.on_click(self.select_files)\n",
    "\n",
    "    @staticmethod\n",
    "    def select_files(b):\n",
    "        \"\"\"Generate instance of tkinter.filedialog.\n",
    "        Parameters\n",
    "        ----------\n",
    "        b : obj:\n",
    "            An instance of ipywidgets.widgets.Button\n",
    "        \"\"\"\n",
    "        # Create Tk root\n",
    "        root = Tk()\n",
    "        # Hide the main window\n",
    "        root.withdraw()\n",
    "        # Raise the root to the top of all windows.\n",
    "        root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "        # List of selected fileswill be set to b.value\n",
    "        b.files = filedialog.askopenfilename(multiple=True)\n",
    "\n",
    "        b.description = \"Files Selected\"\n",
    "        b.icon = \"check-square-o\"\n",
    "        b.style.button_color = \"lightgreen\"\n",
    "my_button = SelectFilesButton()\n",
    "my_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(my_button.files[0])\n",
    "sentence_dict = {}\n",
    "for i, row in data_frame.iterrows():\n",
    "    if row['top_label'] not in sentence_dict:\n",
    "        sentence_dict[row['top_label']] = []\n",
    "    sentence_dict[row['top_label']].append(row['Input.text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_it_all(sentence_list):\n",
    "    data_words = list(sent_to_words(sentence_list))\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=10) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=10)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops, bigram_mod)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "        # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=1, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    return coherence_lda, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_coher_dict = {}\n",
    "for key in sentence_dict:\n",
    "    perplexity, coherence = do_it_all(sentence_dict[key])\n",
    "    perp_coher_dict[key] = (perplexity, coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Everyday Decision Making': (0.6746817714435754, -5.7390281318703575),\n",
       " 'Work': (0.4910513589427044, -5.78980511883024),\n",
       " 'Other': (0.6646450695488907, -6.1177688476188194),\n",
       " 'Financial Problem': (0.4249728412042283, -5.377070625782013),\n",
       " 'Health, Fatigue, or Physical Pain': (0.6063830948312298, -5.606957616768484),\n",
       " 'Emotional Turmoil': (0.6733387787741211, -5.311907352650002),\n",
       " 'Family Issues': (0.521829905760556, -5.8794804003364165),\n",
       " 'Social Relationships': (0.6661313537669303, -5.611232774202214),\n",
       " 'School': (0.5562748114676086, -5.001007715600436)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perp_coher_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the original numbers I sent you, but the bigrams were incorrect because the bigrams were based off of the whole set and not just the set of the sentences themselves. The numbers from the dictionary above are doing what the function should do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Everyday Decision Making': (0.5174985290681113, -6.692375377518451),\n",
       " 'Work': (0.621771529270715, -6.362156521523999),\n",
       " 'Other': (0.6288803228187319, -6.871340214920805),\n",
       " 'Financial Problem': (0.5560582210030645, -6.019651740617766),\n",
       " 'Health, Fatigue, or Physical Pain': (0.5232453446318873, -6.411790882606828),\n",
       " 'Emotional Turmoil': (0.4562448301898699, -6.121067112292385),\n",
       " 'Family Issues': (0.5621992013745302, -6.5610873569118775),\n",
       " 'Social Relationships': (0.5477731193303987, -6.4972188039317),\n",
       " 'School': (0.5759366775501313, -5.764681655714909)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perp_coher_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
