{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import f1_score,confusion_matrix,classification_report,accuracy_score\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples_prediction(df):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        #labels = row[LABEL_HOT_VECTOR].strip('][').split(', ')\n",
    "        #labels = [float(x) for x in labels]\n",
    "        labels = list(row[label_list_text])\n",
    "        examples.append(labels)\n",
    "        \n",
    "    return pd.DataFrame(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    n = 2  # index of the second proability to get labeled \n",
    "\n",
    "    index = np.argsort(x.values.flatten().tolist())[-n:][0]\n",
    "    print(f\"index is {index}\")\n",
    "    label  = label_list_text[index]\n",
    "    print(f\"label is {label}\")\n",
    "    \n",
    "    return label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_experiment_df(test):\n",
    "    test_predictions = [x[0]['probabilities'] for x in zip(getListPrediction(in_sentences=list(test[DATA_COLUMN])))]\n",
    "    test_live_labels = np.array(test_predictions).argmax(axis=1)\n",
    "    test['Predicted label'] = [label_list_text[x] for x in test_live_labels] # appending the labels to the dataframe\n",
    "    \n",
    "    probabilities_df_live = pd.DataFrame(test_predictions) # creating a proabilities dataset\n",
    "    probabilities_df_live.columns = [x + \" Predicted\"for x in label_list_text] # naming the columns\n",
    "    probabilities_df_live['Predicted label 2'] = probabilities_df_live.apply(lambda x:f(x),axis=1)\n",
    "    \n",
    "    #print(test)\n",
    "    #label_df = create_examples_prediction(test)\n",
    "    #label_df.columns = label_list_text\n",
    "    #label_df['label 2'] = label_df.apply(lambda x:f(x),axis=1)\n",
    "\n",
    "    test.reset_index(inplace=True,drop=True) # resetting index \n",
    "\n",
    "    experiment_df = pd.concat([test,probabilities_df_live],axis=1, ignore_index=False)\n",
    "    experiment_df = experiment_df.reindex(sorted(experiment_df.columns), axis=1)\n",
    "    return test,experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListPrediction(in_sentences):\n",
    "    #1\n",
    "    input_examples = [InputExample(guid=\"\", text_a = x, text_b = None, labels = [0]*len(label_list)) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "    \n",
    "    #2\n",
    "    input_features = convert_examples_to_features(input_examples, MAX_SEQ_LENGTH, tokenizer)\n",
    "    \n",
    "    #3\n",
    "    predict_input_fn = input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    \n",
    "    print(input_features[0].input_ids)\n",
    "    #4\n",
    "    predictions = estimator.predict(input_fn=predict_input_fn,yield_single_examples=True)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_normalize_active=False\n",
    "\n",
    "def get_confusion_matrix(y_test,predicted,labels):\n",
    "    class_names=labels\n",
    "    # plotting confusion matrix\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_confusion_matrix(y_test, predicted, classes=class_names,\n",
    "                        title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plot_confusion_matrix(y_test, predicted, classes=class_names, normalize=True,\n",
    "                        title='Normalized confusion matrix')\n",
    "    plt.show()\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes =classes\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        test =1\n",
    "        #print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #fig.tight_layout()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_bert(df,test_size):\n",
    "    \n",
    "    #print(\"Filling missing values\")\n",
    "    #df[DATA_COLUMN] = df[DATA_COLUMN].fillna('_NA_')\n",
    "    \n",
    "    print(\"Splitting dataframe with shape {} into training and test datasets\".format(df.shape))\n",
    "    X_train, X_test  = train_test_split(df, test_size=test_size, random_state=2018,stratify = df[LABEL_COLUMN_RAW])\n",
    "\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dataset(NAME,mapping_index,excluded_categories):\n",
    "    df = pd.read_csv(PATH+NAME+'.csv',sep =',')\n",
    "    \n",
    "    #df[LABEL_COLUMN_RAW] = df[LABEL_COLUMN_RAW].fillna(\"Other\")\n",
    "\n",
    "    \n",
    "    df = df[df['is_stressor'] == 1]\n",
    "    df = df[df[LABEL_COLUMN_RAW] != 'Not Stressful']\n",
    "    #df.columns = [LABEL_COLUMN_RAW,'Severity',DATA_COLUMN,'Source']\n",
    "    \n",
    "    if excluded_categories is not None:\n",
    "        for category in excluded_categories:\n",
    "\n",
    "            df = df[df[LABEL_COLUMN_RAW] !=category]\n",
    "\n",
    "    label_list=[]\n",
    "    label_list_final =[]\n",
    "    if(mapping_index is None):\n",
    "        df[LABEL_COLUMN_RAW] = df[LABEL_COLUMN_RAW].astype('category')\n",
    "        df[LABEL_COLUMN], mapping_index = pd.Series(df[LABEL_COLUMN_RAW]).factorize() #uses pandas factorize() to convert to numerical index\n",
    "        \n",
    "  \n",
    "    else:\n",
    "        df[LABEL_COLUMN] = df[LABEL_COLUMN_RAW].apply(lambda x: mapping_index.get_loc(x))\n",
    "    \n",
    "    label_list_final = [None] * len(mapping_index.categories)\n",
    "    label_list_number = [None] * len(mapping_index.categories)\n",
    "\n",
    "    for index,ele in enumerate(list(mapping_index.categories)):\n",
    "        lindex = mapping_index.get_loc(ele)\n",
    "        label_list_number[lindex] = lindex\n",
    "        label_list_final[lindex] = ele\n",
    "    \n",
    "    frequency_dict = df[LABEL_COLUMN_RAW].value_counts().to_dict()\n",
    "    df[\"class_freq\"] = df[LABEL_COLUMN_RAW].apply(lambda x: frequency_dict[x])\n",
    "    \n",
    "    \n",
    "    return df,mapping_index,label_list_number,label_list_final\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Require user changes > Start Here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './datasets/'\n",
    "TODAY_DATE = \"01_05_2020/\"\n",
    "EXPERIMENT_NAME = 'main_turk_analysis_of_5_turkers_popbots_test_live_10votes'\n",
    "EXPERIMENTS_PATH = PATH + 'experiments/'+TODAY_DATE+EXPERIMENT_NAME\n",
    "if not os.path.exists(PATH + 'experiments/'+TODAY_DATE):\n",
    "    os.mkdir(PATH + 'experiments/'+TODAY_DATE)\n",
    "if not os.path.exists(EXPERIMENTS_PATH):\n",
    "    os.mkdir(EXPERIMENTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "# We'll set sequences to be at most 32 tokens long.\n",
    "MAX_SEQ_LENGTH = 32\n",
    "\n",
    "\n",
    "OUTPUT_DIR = './models/'+EXPERIMENT_NAME+ '/' #_01_04_2020/\n",
    "\n",
    "##use downloaded model, change path accordingly\n",
    "BERT_VOCAB= './bert_model/uncased_L-12_H-768_A-12/vocab.txt'\n",
    "BERT_INIT_CHKPNT = './bert_model/uncased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "BERT_CONFIG = './bert_model/uncased_L-12_H-768_A-12/bert_config.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = '2020-04-28-Main-turk-aggregation-5-turkers'\n",
    "\n",
    "DATA_COLUMN = 'Input.text'\n",
    "LABEL_COLUMN_RAW = 'labels'#'Answer.Label'\n",
    "LABEL_COLUMN = 'label_numeric'\n",
    "\n",
    "MTURK_NAME = 'mTurk_synthetic'\n",
    "LIVE_NAME = 'popbots_live'\n",
    "\n",
    "LABEL_HOT_VECTOR = 'label_conf'\n",
    "\n",
    "#dataset,mapping_index,label_list, label_list_text = open_dataset('mturk900balanced',None)\n",
    "\n",
    "EXCLUDED_CATEGORIES = None #['Other'] #None # # if nothing to exclude put None, THIS ALWAYS MUST BE A LIST \n",
    "mapping_dict = {'Other': 0, 'Everyday Decision Making': 1, 'Work': 2, 'Social Relationships': 3, 'Financial Problem': 4, 'Emotional Turmoil': 5, 'Health, Fatigue, or Physical Pain': 6, 'School': 7, 'Family Issues': 8}#,'Not Stressful':9}\n",
    "mapping_index = pd.CategoricalIndex([key for key,value in mapping_dict.items()])\n",
    "\n",
    "dataset,mapping_index,label_list, label_list_text = open_dataset(DATASET_NAME,mapping_index,EXCLUDED_CATEGORIES)\n",
    "\n",
    "#dataset = dataset[dataset['is_stressor'] == 1]\n",
    "\n",
    "test_on_mturk_and_popbots_live = True # include live data in training + include mturk in testing\n",
    "\n",
    "\n",
    "if test_on_mturk_and_popbots_live:\n",
    "    \n",
    "    mturk = dataset[dataset['Source']== MTURK_NAME]\n",
    "    live = dataset[dataset['Source']== LIVE_NAME]\n",
    "    live = live.sample(frac=1).reset_index(drop=True) # shuffle live\n",
    "    \n",
    "    PERCENTAGE_LIVE_TEST = 70\n",
    "    \n",
    "    TEST_PERCENTAGE = len(live)/((100/PERCENTAGE_LIVE_TEST)*len(mturk))  # given to set the percentage of mturk used as test set to have 50/50\n",
    "    \n",
    "    print(f\"Test percentage is {TEST_PERCENTAGE}\")\n",
    "\n",
    "    train,test = data_prep_bert(mturk,TEST_PERCENTAGE) # test size from mturk \n",
    "    \n",
    "    train = train.append(live.loc[0:int((1-(PERCENTAGE_LIVE_TEST/100))*len(live))]) # taking 1/2 of that dataset for training\n",
    "    \n",
    "    test = test.append(live.loc[int(len(live)*(1-(PERCENTAGE_LIVE_TEST/100))):int(len(live))]) # taking 1/2 of live dataset for testing\n",
    "else:\n",
    "    # or taking live only for testing\n",
    "    train,test = dataset[dataset['Source']== MTURK_NAME],dataset[dataset['Source']== LIVE_NAME] \n",
    "\n",
    "#train = train[train['is_stressor'] == 1] # remove only non stressor from train\n",
    "\n",
    "#print(f\"Dataset has {len(dataset)} training examples\")\n",
    "print(f\"Normal label list is {label_list}\")\n",
    "print(f\"The labels text is {label_list_text}\")\n",
    "\n",
    "#Export train test to csv\n",
    "#train.to_csv(PATH+'900_CSV_SPLITTED/train.csv')\n",
    "#test.to_csv(PATH+'900_CSV_SPLITTED/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_columns = ['category', 'nb_sentence','nb_sentence_sampled','mean_distinct_word_nb','mean_distinc_word_per_sentence', 'sd','sd per sentence', '95conf_int' ]\n",
    "count_results = pd.DataFrame(columns = df_columns)\n",
    "\n",
    "boostrap_number = 50\n",
    "for category in label_list_text:\n",
    "    len_word_distinct_word = 0\n",
    "    len_word_distinct_word_list = []\n",
    "    for i in range(boostrap_number):\n",
    "        category_df = dataset[dataset[LABEL_COLUMN_RAW] == category].sample(n=38)\n",
    "        category_df_unsampled = dataset[dataset[LABEL_COLUMN_RAW] == category]\n",
    "        results = set()\n",
    "        category_df[DATA_COLUMN].str.lower().str.split().apply(results.update)\n",
    "        \n",
    "        len_word_distinct_word += len(list(results))\n",
    "        len_word_distinct_word_list.append(len(list(results)))\n",
    "    count_results = count_results.append({'category':category,\n",
    "                                          'nb_sentence':len(category_df_unsampled),\n",
    "                                          'nb_sentence_sampled':len(category_df),'mean_distinct_word_nb':len_word_distinct_word/boostrap_number,\n",
    "                                          'mean_distinc_word_per_sentence':len_word_distinct_word/boostrap_number/len(category_df),'sd':0,\n",
    "                                          'list_distinct':len_word_distinct_word_list},\n",
    "                                         ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_conf_interval(stats):\n",
    "    alpha = 0.95\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    lower = max(0.0, np.percentile(stats, p))\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    upper = max(1.0, np.percentile(stats, p))\n",
    "    \n",
    "    return [\"{0:.2f}\".format(lower),\"{0:.2f}\".format(upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>nb_sentence</th>\n",
       "      <th>nb_sentence_sampled</th>\n",
       "      <th>mean_distinct_word_nb</th>\n",
       "      <th>mean_distinc_word_per_sentence</th>\n",
       "      <th>sd</th>\n",
       "      <th>sd per sentence</th>\n",
       "      <th>95conf_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>School</td>\n",
       "      <td>158</td>\n",
       "      <td>38</td>\n",
       "      <td>167.92</td>\n",
       "      <td>4.418947</td>\n",
       "      <td>10.983333</td>\n",
       "      <td>0.289035</td>\n",
       "      <td>[3.92, 5.02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Financial Problem</td>\n",
       "      <td>339</td>\n",
       "      <td>38</td>\n",
       "      <td>183.78</td>\n",
       "      <td>4.836316</td>\n",
       "      <td>11.824196</td>\n",
       "      <td>0.311163</td>\n",
       "      <td>[4.08, 5.40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Health, Fatigue, or Physical Pain</td>\n",
       "      <td>129</td>\n",
       "      <td>38</td>\n",
       "      <td>185.94</td>\n",
       "      <td>4.893158</td>\n",
       "      <td>10.490777</td>\n",
       "      <td>0.276073</td>\n",
       "      <td>[4.30, 5.33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Work</td>\n",
       "      <td>709</td>\n",
       "      <td>38</td>\n",
       "      <td>187.20</td>\n",
       "      <td>4.926316</td>\n",
       "      <td>13.284578</td>\n",
       "      <td>0.349594</td>\n",
       "      <td>[4.12, 5.62]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everyday Decision Making</td>\n",
       "      <td>116</td>\n",
       "      <td>38</td>\n",
       "      <td>194.38</td>\n",
       "      <td>5.115263</td>\n",
       "      <td>12.132419</td>\n",
       "      <td>0.319274</td>\n",
       "      <td>[4.55, 5.70]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other</td>\n",
       "      <td>111</td>\n",
       "      <td>38</td>\n",
       "      <td>198.70</td>\n",
       "      <td>5.228947</td>\n",
       "      <td>11.331814</td>\n",
       "      <td>0.298206</td>\n",
       "      <td>[4.80, 5.94]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Social Relationships</td>\n",
       "      <td>127</td>\n",
       "      <td>38</td>\n",
       "      <td>199.68</td>\n",
       "      <td>5.254737</td>\n",
       "      <td>11.455025</td>\n",
       "      <td>0.301448</td>\n",
       "      <td>[4.58, 5.79]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Emotional Turmoil</td>\n",
       "      <td>76</td>\n",
       "      <td>38</td>\n",
       "      <td>207.82</td>\n",
       "      <td>5.468947</td>\n",
       "      <td>11.564930</td>\n",
       "      <td>0.304340</td>\n",
       "      <td>[4.89, 6.00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Family Issues</td>\n",
       "      <td>235</td>\n",
       "      <td>38</td>\n",
       "      <td>208.44</td>\n",
       "      <td>5.485263</td>\n",
       "      <td>12.236274</td>\n",
       "      <td>0.322007</td>\n",
       "      <td>[4.83, 6.20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            category nb_sentence nb_sentence_sampled  \\\n",
       "7                             School         158                  38   \n",
       "4                  Financial Problem         339                  38   \n",
       "6  Health, Fatigue, or Physical Pain         129                  38   \n",
       "2                               Work         709                  38   \n",
       "1           Everyday Decision Making         116                  38   \n",
       "0                              Other         111                  38   \n",
       "3               Social Relationships         127                  38   \n",
       "5                  Emotional Turmoil          76                  38   \n",
       "8                      Family Issues         235                  38   \n",
       "\n",
       "   mean_distinct_word_nb  mean_distinc_word_per_sentence         sd  \\\n",
       "7                 167.92                        4.418947  10.983333   \n",
       "4                 183.78                        4.836316  11.824196   \n",
       "6                 185.94                        4.893158  10.490777   \n",
       "2                 187.20                        4.926316  13.284578   \n",
       "1                 194.38                        5.115263  12.132419   \n",
       "0                 198.70                        5.228947  11.331814   \n",
       "3                 199.68                        5.254737  11.455025   \n",
       "5                 207.82                        5.468947  11.564930   \n",
       "8                 208.44                        5.485263  12.236274   \n",
       "\n",
       "   sd per sentence    95conf_int  \n",
       "7         0.289035  [3.92, 5.02]  \n",
       "4         0.311163  [4.08, 5.40]  \n",
       "6         0.276073  [4.30, 5.33]  \n",
       "2         0.349594  [4.12, 5.62]  \n",
       "1         0.319274  [4.55, 5.70]  \n",
       "0         0.298206  [4.80, 5.94]  \n",
       "3         0.301448  [4.58, 5.79]  \n",
       "5         0.304340  [4.89, 6.00]  \n",
       "8         0.322007  [4.83, 6.20]  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "count_results['sd']= count_results['list_distinct'].apply(lambda x:np.std(np.array(x), axis=0))\n",
    "count_results['sd per sentence']= count_results['sd']/38\n",
    "\n",
    "\n",
    "count_results['95conf_int']= count_results['list_distinct'].apply(lambda x:return_conf_interval([x/38 for x in x]))\n",
    "\n",
    "\n",
    "count_results[df_columns].sort_values(by=['mean_distinc_word_per_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-119-a493078d9ee8>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-119-a493078d9ee8>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    ignore_index=True)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_columns = ['category', 'nb_sentence','nb_sentence_sampled','mean_distinct_word_nb','mean_distinc_word_per_sentence', 'sd','sd per sentence', '95conf_int' ]\n",
    "count_results = pd.DataFrame(columns = df_columns)\n",
    "\n",
    "boostrap_number = 50\n",
    "for category in label_list_text:\n",
    "    len_word_distinct_word = 0\n",
    "    len_word_distinct_word_list = []\n",
    "    for i in range(boostrap_number):\n",
    "        category_df = dataset[dataset[LABEL_COLUMN_RAW] == category].sample(n=38)\n",
    "        category_df_unsampled = dataset[dataset[LABEL_COLUMN_RAW] == category]\n",
    "        \n",
    "        category_df['embedding'] = model.encode(category_df[DATA_COLUMN].values)\n",
    "        \n",
    "        average_sd = category_df['embedding'].std(axis=0) # vector of 768 dim\n",
    "        overall_sd = average_sd.std(axis=1) # overall sd\n",
    "        \n",
    "    count_results = count_results.append({'category':category,\n",
    "                                          'nb_sentence':len(category_df_unsampled),\n",
    "                                          'nb_sentence_sampled':len(category_df),\n",
    "                                          'overall_sd':float(overall_sd)\n",
    "                                         ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set and test set analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(train,test):\n",
    "    print(f\"Train size {len(train)} with {len(train[train['Source']== LIVE_NAME])} from Popbots and {len(train[train['Source']== MTURK_NAME])} from mturk\")\n",
    "    print(f\"Test size {len(test)} with {len(test[test['Source']== LIVE_NAME])} from Popbots and {len(test[test['Source']== MTURK_NAME])} from mturk\")\n",
    "    \n",
    "    print('\\nTraining distribution:')\n",
    "    print(pd.pivot_table(train[[LABEL_COLUMN_RAW, 'Source']],index=[LABEL_COLUMN_RAW, 'Source'],columns=None, aggfunc=len)) #.to_clipboard(excel=True)\n",
    "          \n",
    "    print('\\nTesting distribution:')\n",
    "    print(pd.pivot_table(test[[LABEL_COLUMN_RAW, 'Source']],index=[LABEL_COLUMN_RAW, 'Source'],columns=None, aggfunc=len)) #.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac=1).reset_index(drop=True) #reshuffle everything\n",
    "test = test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All dataset distribution:\n",
      "labels                             Source         \n",
      "Emotional Turmoil                  mTurk_synthetic     70\n",
      "                                   popbots_live         6\n",
      "Everyday Decision Making           mTurk_synthetic     99\n",
      "                                   popbots_live        17\n",
      "Family Issues                      mTurk_synthetic    225\n",
      "                                   popbots_live        10\n",
      "Financial Problem                  mTurk_synthetic    337\n",
      "                                   popbots_live         2\n",
      "Health, Fatigue, or Physical Pain  mTurk_synthetic    109\n",
      "                                   popbots_live        20\n",
      "Other                              mTurk_synthetic     97\n",
      "                                   popbots_live        14\n",
      "School                             mTurk_synthetic    145\n",
      "                                   popbots_live        13\n",
      "Social Relationships               mTurk_synthetic    113\n",
      "                                   popbots_live        14\n",
      "Work                               mTurk_synthetic    670\n",
      "                                   popbots_live        39\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('\\nAll dataset distribution:')\n",
    "print(pd.pivot_table(dataset[[LABEL_COLUMN_RAW, 'Source']],index=[LABEL_COLUMN_RAW, 'Source'],columns=None, aggfunc=len)) #.to_clipboard(excel=T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 1811 with 41 from Popbots and 1770 from mturk\n",
      "Test size 190 with 95 from Popbots and 95 from mturk\n",
      "\n",
      "Training distribution:\n",
      "labels                             Source         \n",
      "Emotional Turmoil                  mTurk_synthetic     66\n",
      "                                   popbots_live         2\n",
      "Everyday Decision Making           mTurk_synthetic     94\n",
      "                                   popbots_live         4\n",
      "Family Issues                      mTurk_synthetic    214\n",
      "                                   popbots_live         2\n",
      "Financial Problem                  mTurk_synthetic    320\n",
      "Health, Fatigue, or Physical Pain  mTurk_synthetic    103\n",
      "                                   popbots_live         6\n",
      "Other                              mTurk_synthetic     92\n",
      "                                   popbots_live         5\n",
      "School                             mTurk_synthetic    138\n",
      "                                   popbots_live         3\n",
      "Social Relationships               mTurk_synthetic    107\n",
      "                                   popbots_live         3\n",
      "Work                               mTurk_synthetic    636\n",
      "                                   popbots_live        16\n",
      "dtype: int64\n",
      "\n",
      "Testing distribution:\n",
      "labels                             Source         \n",
      "Emotional Turmoil                  mTurk_synthetic     4\n",
      "                                   popbots_live        4\n",
      "Everyday Decision Making           mTurk_synthetic     5\n",
      "                                   popbots_live       13\n",
      "Family Issues                      mTurk_synthetic    11\n",
      "                                   popbots_live        8\n",
      "Financial Problem                  mTurk_synthetic    17\n",
      "                                   popbots_live        2\n",
      "Health, Fatigue, or Physical Pain  mTurk_synthetic     6\n",
      "                                   popbots_live       15\n",
      "Other                              mTurk_synthetic     5\n",
      "                                   popbots_live        9\n",
      "School                             mTurk_synthetic     7\n",
      "                                   popbots_live       10\n",
      "Social Relationships               mTurk_synthetic     6\n",
      "                                   popbots_live       11\n",
      "Work                               mTurk_synthetic    34\n",
      "                                   popbots_live       23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_dataset_info(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step to reduce the most dominant categories and balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_cutoff = 100 # all the categories which had less than 100 example won't be sampled down\n",
    "total_training_size = 1501\n",
    "\n",
    "REVERSE_FREQ = 'Max_reverse_sampling_chance'\n",
    "train[REVERSE_FREQ] = train['class_freq'].apply(lambda x: (max(train['class_freq'])/x)) \n",
    "\n",
    "sampling_boolean = (train['Source'] != LIVE_NAME) & (train['class_freq'].astype(float) > sampling_cutoff) \n",
    "\n",
    "\n",
    "train_to_be_balanced = train[sampling_boolean]\n",
    "train_not_resampled = train[~sampling_boolean]\n",
    "\n",
    "train_temp = train_to_be_balanced.sample(n=(total_training_size-len(train_not_resampled)), weights=REVERSE_FREQ, random_state=2020)\n",
    "train = pd.concat([train_temp,train_not_resampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 1501 with 0 from Popbots and 1501 from mturk\n",
      "Test size 135 with 135 from Popbots and 0 from mturk\n",
      "\n",
      "Training distribution:\n",
      "labels                             Source         \n",
      "Emotional Turmoil                  mTurk_synthetic     70\n",
      "Everyday Decision Making           mTurk_synthetic     99\n",
      "Family Issues                      mTurk_synthetic    209\n",
      "Financial Problem                  mTurk_synthetic    273\n",
      "Health, Fatigue, or Physical Pain  mTurk_synthetic    109\n",
      "Other                              mTurk_synthetic     97\n",
      "School                             mTurk_synthetic    140\n",
      "Social Relationships               mTurk_synthetic    113\n",
      "Work                               mTurk_synthetic    391\n",
      "dtype: int64\n",
      "\n",
      "Testing distribution:\n",
      "labels                             Source      \n",
      "Emotional Turmoil                  popbots_live     6\n",
      "Everyday Decision Making           popbots_live    17\n",
      "Family Issues                      popbots_live    10\n",
      "Financial Problem                  popbots_live     2\n",
      "Health, Fatigue, or Physical Pain  popbots_live    20\n",
      "Other                              popbots_live    14\n",
      "School                             popbots_live    13\n",
      "Social Relationships               popbots_live    14\n",
      "Work                               popbots_live    39\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_dataset_info(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalIndex(['Other', 'Everyday Decision Making', 'Work',\n",
       "                  'Social Relationships', 'Financial Problem',\n",
       "                  'Emotional Turmoil', 'Health, Fatigue, or Physical Pain',\n",
       "                  'School', 'Family Issues'],\n",
       "                 categories=['Emotional Turmoil', 'Everyday Decision Making', 'Family Issues', 'Financial Problem', 'Health, Fatigue, or Physical Pain', 'Other', 'School', 'Social Relationships', ...], ordered=False, dtype='category')"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(EXPERIMENTS_PATH+'/TRAIN_'+DATASET_NAME+'.csv')\n",
    "test.to_csv(EXPERIMENTS_PATH+'/TEST_'+DATASET_NAME+'.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting Movie Reviews with BERT on TF Hub.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
